{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b74cc13",
   "metadata": {},
   "source": [
    "\n",
    "# DS1 Spark + Python Prep Notebook\n",
    "\n",
    "This notebook is designed as a **comprehensive but focused** preparation for a **Data Scientist 1** interview that uses:\n",
    "\n",
    "- PySpark (DataFrames)\n",
    "- SQL-style data work\n",
    "- Basic Python\n",
    "\n",
    "It assumes:\n",
    "- You have a `SparkSession` named `spark` (Databricks / EMR / local)\n",
    "- You are working with a CSV-like dataset (e.g., retail transactions)\n",
    "\n",
    "You do **not** need deep ML or graph databases for this prep. The emphasis is on **data manipulation, querying, and reasoning**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa7537",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "In Databricks or most managed Spark environments, a `SparkSession` named `spark` is already available.\n",
    "\n",
    "If you're running this locally in Jupyter, you may need to create it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, instr, lit, when,\n",
    "    count, sum as _sum, avg, max as _max, min as _min,\n",
    "    row_number, rank, dense_rank, desc, asc\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# If Spark is not yet created (e.g., local Jupyter), uncomment:\n",
    "# spark = SparkSession.builder #     .appName(\"ds1_spark_prep_notebook\") #     .getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b328c22e",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Loading Data as a DataFrame\n",
    "\n",
    "**Key interview idea:**  \n",
    "- A CSV is a **file on disk** (raw bytes).  \n",
    "- A **DataFrame** is a **structured, distributed table** in Spark, created *after* reading the file.\n",
    "\n",
    "Below, adjust the path to point to a CSV on your system. If you have the Spark Definitive Guide data, you can use something like `data/retail-data/by-day/2010-12-01.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b508c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Update this path to a real CSV on your system.\n",
    "csv_path = \"data/retail-data/by-day/2010-12-01.csv\"\n",
    "\n",
    "df = spark.read.csv(\n",
    "    csv_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa04e61",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Basic DataFrame Operations\n",
    "\n",
    "Core skills:\n",
    "- `select` specific columns\n",
    "- Create new columns with `withColumn`\n",
    "- Filter rows with `filter` / `where`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528cc04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select a subset of columns\n",
    "df.select(\"InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"UnitPrice\").show(5, truncate=False)\n",
    "\n",
    "# Add a computed column: TotalCost = UnitPrice * Quantity\n",
    "df_with_total = df.withColumn(\"TotalCost\", col(\"UnitPrice\") * col(\"Quantity\"))\n",
    "df_with_total.select(\"InvoiceNo\", \"StockCode\", \"Quantity\", \"UnitPrice\", \"TotalCost\").show(5, truncate=False)\n",
    "\n",
    "# Filter: UnitPrice greater than 10\n",
    "df_with_total.filter(col(\"UnitPrice\") > 10).select(\"InvoiceNo\", \"UnitPrice\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddbf2e6",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Boolean Filters and String Matching\n",
    "\n",
    "This section mirrors the pattern you remember:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import instr\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a15152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "# Boolean conditions\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "\n",
    "# Chain where() conditions and use boolean OR\n",
    "filtered = df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter)\n",
    "\n",
    "filtered.select(\"InvoiceNo\", \"StockCode\", \"Description\", \"UnitPrice\").show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6b7e0",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Handling NULLs\n",
    "\n",
    "Common tasks:\n",
    "- Count NULLs\n",
    "- Drop rows with critical NULLs\n",
    "- Fill NULLs with default values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d39cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count rows where Description is NULL\n",
    "df.select(\n",
    "    count(when(col(\"Description\").isNull(), 1)).alias(\"NullDescriptionCount\")\n",
    ").show()\n",
    "\n",
    "# Drop rows where key columns are NULL\n",
    "df_clean = df.dropna(subset=[\"InvoiceNo\", \"StockCode\", \"Quantity\", \"UnitPrice\"])\n",
    "print(\"Original count:\", df.count())\n",
    "print(\"After dropna count:\", df_clean.count())\n",
    "\n",
    "# Fill NULL descriptions with a default label\n",
    "df_filled = df_clean.fillna({\"Description\": \"UNKNOWN\"})\n",
    "df_filled.select(\"InvoiceNo\", \"StockCode\", \"Description\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc0c6cc",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Aggregations with `groupBy` and `agg`\n",
    "\n",
    "Core interview operations:\n",
    "- `groupBy` one or more keys\n",
    "- `agg` with functions like `sum`, `avg`, `count`\n",
    "- `orderBy` results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_with_total = df.withColumn(\"TotalCost\", col(\"UnitPrice\") * col(\"Quantity\"))\n",
    "\n",
    "agg_by_stock = df_with_total.groupBy(\"StockCode\").agg(\n",
    "    _sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
    "    _sum(\"TotalCost\").alias(\"TotalRevenue\"),\n",
    "    avg(\"UnitPrice\").alias(\"AvgUnitPrice\")\n",
    ")\n",
    "\n",
    "agg_by_stock.orderBy(desc(\"TotalRevenue\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b8e5d",
   "metadata": {},
   "source": [
    "\n",
    "### 6.1 Multiple Grouping Keys\n",
    "\n",
    "You can group by multiple columns, e.g., `CustomerID` + `Country`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example assuming df has CustomerID and Country (some versions of the retail dataset do)\n",
    "group_cols = [c for c in df.columns if c in [\"CustomerID\", \"Country\"]]\n",
    "print(\"Possible group columns detected:\", group_cols)\n",
    "\n",
    "if \"CustomerID\" in group_cols and \"Country\" in group_cols:\n",
    "    agg_by_customer_country = df_with_total.groupBy(\"CustomerID\", \"Country\").agg(\n",
    "        _sum(\"TotalCost\").alias(\"TotalSpend\")\n",
    "    )\n",
    "    agg_by_customer_country.orderBy(desc(\"TotalSpend\")).show(10, truncate=False)\n",
    "else:\n",
    "    print(\"CustomerID / Country not found in this dataset; adjust for your schema.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292cbecc",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Joins\n",
    "\n",
    "Key join concepts they might ask:\n",
    "- INNER vs LEFT joins\n",
    "- Joining on a key column\n",
    "- Understanding row explosion / duplication\n",
    "\n",
    "We’ll create small example DataFrames to practice the pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444611d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Small example: customers and countries\n",
    "customers = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"C001\", \"UK\"),\n",
    "        (2, \"C002\", \"Germany\"),\n",
    "        (3, \"C003\", \"France\"),\n",
    "    ],\n",
    "    [\"CustomerPK\", \"CustomerID\", \"CountryCode\"]\n",
    ")\n",
    "\n",
    "countries = spark.createDataFrame(\n",
    "    [\n",
    "        (\"UK\", \"United Kingdom\"),\n",
    "        (\"Germany\", \"Germany\"),\n",
    "        (\"France\", \"France\"),\n",
    "    ],\n",
    "    [\"CountryCode\", \"CountryName\"]\n",
    ")\n",
    "\n",
    "customers.show()\n",
    "countries.show()\n",
    "\n",
    "# LEFT join on CountryCode\n",
    "customers_with_country = customers.join(\n",
    "    countries,\n",
    "    on=\"CountryCode\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "customers_with_country.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef16de7",
   "metadata": {},
   "source": [
    "\n",
    "### 7.1 Joining with a Fact Table\n",
    "\n",
    "Pattern:\n",
    "1. Aggregate a fact table (e.g., transactions by `CustomerID`)\n",
    "2. Join with a dimension (e.g., customer master or country table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: total spend per CustomerID from main df\n",
    "if \"CustomerID\" in df.columns:\n",
    "    df_with_total = df.withColumn(\"TotalCost\", col(\"UnitPrice\") * col(\"Quantity\"))\n",
    "\n",
    "    customer_spend = df_with_total.groupBy(\"CustomerID\").agg(\n",
    "        _sum(\"TotalCost\").alias(\"TotalSpend\")\n",
    "    )\n",
    "\n",
    "    # Join with customers table (on CustomerID)\n",
    "    customer_spend_joined = customer_spend.join(\n",
    "        customers,\n",
    "        on=\"CustomerID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    customer_spend_joined.orderBy(desc(\"TotalSpend\")).show(10, truncate=False)\n",
    "else:\n",
    "    print(\"CustomerID not found in df; adjust join logic for your real schema.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49065dc",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Window Functions (Ranking Within Groups)\n",
    "\n",
    "Common pattern: **“For each X, find the top Y by metric M.”**  \n",
    "Example: For each customer, find their most expensive purchase.\n",
    "\n",
    "Tools:\n",
    "- `Window.partitionBy(...).orderBy(...)`\n",
    "- `row_number`, `rank`, `dense_rank`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_with_total = df.withColumn(\"TotalCost\", col(\"UnitPrice\") * col(\"Quantity\"))\n",
    "\n",
    "if \"CustomerID\" in df.columns:\n",
    "    window_spec = Window.partitionBy(\"CustomerID\").orderBy(desc(\"TotalCost\"))\n",
    "\n",
    "    df_ranked = df_with_total.withColumn(\"rn\", row_number().over(window_spec))\n",
    "\n",
    "    top_purchase_per_customer = df_ranked.where(col(\"rn\") == 1)\n",
    "\n",
    "    top_purchase_per_customer.select(\"CustomerID\", \"InvoiceNo\", \"TotalCost\").show(10, truncate=False)\n",
    "else:\n",
    "    print(\"CustomerID not found in df; window example limited to global ranking.\")\n",
    "\n",
    "# Global top-N invoices by TotalCost\n",
    "window_spec_global = Window.orderBy(desc(\"TotalCost\"))\n",
    "df_ranked_global = df_with_total.withColumn(\"rn_global\", row_number().over(window_spec_global))\n",
    "df_ranked_global.select(\"InvoiceNo\", \"TotalCost\", \"rn_global\").where(col(\"rn_global\") <= 10).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096fde7",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Spark SQL: Using Temp Views\n",
    "\n",
    "Spark lets you switch between:\n",
    "- DataFrame API (methods like `select`, `filter`, `groupBy`)\n",
    "- SQL (strings passed to `spark.sql`)\n",
    "\n",
    "They compile to the same engine underneath.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ccb412",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_with_total.createOrReplaceTempView(\"purchases\")\n",
    "\n",
    "top_customers_sql = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CustomerID,\n",
    "        SUM(UnitPrice * Quantity) AS TotalSpend\n",
    "    FROM purchases\n",
    "    WHERE CustomerID IS NOT NULL\n",
    "    GROUP BY CustomerID\n",
    "    ORDER BY TotalSpend DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "top_customers_sql.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d40c5bc",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Data Cleansing with `when` / `otherwise`\n",
    "\n",
    "Example tasks:\n",
    "- Flagging suspicious rows\n",
    "- Creating category labels\n",
    "- Replacing out-of-range values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b982aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: create a flag column for high-value lines\n",
    "df_flags = df_with_total.withColumn(\n",
    "    \"HighValueFlag\",\n",
    "    when(col(\"TotalCost\") > 1000, lit(1)).otherwise(lit(0))\n",
    ")\n",
    "\n",
    "df_flags.select(\"InvoiceNo\", \"TotalCost\", \"HighValueFlag\").orderBy(desc(\"TotalCost\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe217be",
   "metadata": {},
   "source": [
    "\n",
    "## 11. User-Defined Functions (UDFs) – Optional\n",
    "\n",
    "You typically try to stay with **built-in** functions for performance, but it’s good to know UDFs exist.\n",
    "\n",
    "(For DS1, you can mostly just say that UDFs allow custom Python logic when built-ins are not enough, but they can be slower.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def categorize_price(unit_price):\n",
    "    if unit_price is None:\n",
    "        return \"UNKNOWN\"\n",
    "    if unit_price < 1:\n",
    "        return \"cheap\"\n",
    "    elif unit_price < 10:\n",
    "        return \"moderate\"\n",
    "    else:\n",
    "        return \"expensive\"\n",
    "\n",
    "categorize_price_udf = udf(categorize_price, StringType())\n",
    "\n",
    "df_price_cat = df.withColumn(\"PriceCategory\", categorize_price_udf(col(\"UnitPrice\")))\n",
    "df_price_cat.select(\"UnitPrice\", \"PriceCategory\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7545fc",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Python Refresher (Likely Coding Level for DS1)\n",
    "\n",
    "This is about comfort with:\n",
    "- lists\n",
    "- dicts\n",
    "- comprehensions\n",
    "- simple functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d2a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lists and comprehensions\n",
    "nums = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "even_nums = [n for n in nums if n % 2 == 0]\n",
    "squared = [n * n for n in nums]\n",
    "\n",
    "print(\"Even numbers:\", even_nums)\n",
    "print(\"Squared numbers:\", squared)\n",
    "\n",
    "# Dictionaries\n",
    "prices = {\"apple\": 1.0, \"banana\": 0.5, \"orange\": 0.8}\n",
    "prices_with_tax = {k: v * 1.1 for k, v in prices.items()}\n",
    "print(\"Prices with tax:\", prices_with_tax)\n",
    "\n",
    "# Simple utility function\n",
    "def total_price(items, price_lookup):\n",
    "    total = 0.0\n",
    "    for item in items:\n",
    "        total += price_lookup.get(item, 0.0)\n",
    "    return total\n",
    "\n",
    "basket = [\"apple\", \"banana\", \"banana\"]\n",
    "print(\"Basket:\", basket)\n",
    "print(\"Total price:\", total_price(basket, prices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3216f2e0",
   "metadata": {},
   "source": [
    "\n",
    "## 13. High-Level ML Workflow (Verbal Understanding Only)\n",
    "\n",
    "For DS1, you generally just need to **describe** the supervised learning process:\n",
    "\n",
    "1. **Define the problem**  \n",
    "   - Regression (predict a number) or classification (predict a label).\n",
    "\n",
    "2. **Prepare the data**  \n",
    "   - Select features and label.  \n",
    "   - Handle missing values.  \n",
    "   - Encode categoricals if needed.  \n",
    "   - Scale/normalize if appropriate.\n",
    "\n",
    "3. **Split into train and test sets**  \n",
    "   - e.g., 70/30 or 80/20.\n",
    "\n",
    "4. **Choose a model**  \n",
    "   - e.g., logistic regression, random forest, gradient boosted trees.\n",
    "\n",
    "5. **Train the model on the training set**.\n",
    "\n",
    "6. **Evaluate on the test set**  \n",
    "   - Metrics: accuracy, precision/recall, ROC AUC, RMSE, etc.\n",
    "\n",
    "7. **Iterate**  \n",
    "   - Try new features, different models, tuning hyperparameters.\n",
    "\n",
    "You usually do **not** need to derive formulas or gradients for DS1 – just show you understand the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954eb1ae",
   "metadata": {},
   "source": [
    "\n",
    "## 14. What to Be Ready to Explain in an Interview\n",
    "\n",
    "If you can confidently talk through and sketch code for:\n",
    "\n",
    "- How to read a CSV into a DataFrame\n",
    "- How to inspect schema and data\n",
    "- How to `select`, `filter`, `withColumn`\n",
    "- How to aggregate with `groupBy().agg()`\n",
    "- How to `join` two DataFrames\n",
    "- What a window function is used for\n",
    "- How to register a temp view and query with SQL\n",
    "- How to handle NULLs and create flags with `when`\n",
    "- Basic Python list/dict/loop skills\n",
    "- High-level ML workflow\n",
    "\n",
    "…then you are in **very solid shape** for a DS1 interview.\n",
    "\n",
    "Use this notebook as your **sandbox**: run things, break things, and fix them. That process is exactly what real work looks like.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
